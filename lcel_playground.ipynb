{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_RESOURCE\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_NAME\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_VERSION\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    "    max_tokens=4000,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.environ[\"AZURE_EMBEDDINGS_NAME\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_RESOURCE\"],\n",
    "    api_version=os.environ[\"AZURE_EMBEDDINGS_VERSION\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is LCEL?\n",
    "* LangChain expression language that allows to chain together runnable elements using the pipe `|` operator.\n",
    "* When chaining together runnable elements the output of the previous element .invoke() is passed to the next one.\n",
    "* A sequence of chained runnable elements is considered a Runnable Sequence and is itslef a Runnable that can be chained.\n",
    "\n",
    "# Why LangChain recommends the usage of LCEL?\n",
    "\n",
    "* Best for streaming: Get the best time-to-first-token. Get incremental chunks of output for chains at the same token rate as the given by the service provider (API).\n",
    "* Sync and async support.\n",
    "* Parallel execution: When some steps in a chain can be parallelized, they do it automatically.\n",
    "* More summarized code and more representative of the LangChain chains (easier to understand).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG example with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pablo was working in a bakery last summer.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"pablo used to work in a bakery last summer\", \"during winter pablo studies\"],\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt |  chat_model | output_parser\n",
    "\n",
    "chain.invoke(\"Where was Pablo working last summer?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need the final output of a LCEL chain we always have to use the `StrOutputParser`, otherwise we get all the response metadata as output. Using `StrOutputParser` is like only keeping `.content` when using `.invoke()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Pablo was working in a bakery last summer.', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_2f57f81c11', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-557e0166-fd97-43c8-8401-b5fd1825703b-0', usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup_and_retrieval | prompt |  chat_model\n",
    "\n",
    "chain.invoke(\"Where was Pablo working last summer?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "It is done in the same way, we just use the runnable chain to run .astream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pablo spent his last summer enveloped in the warm, comforting aroma of freshly baked bread and pastries. Each morning, as the sun began to peek over the horizon, he would tie his apron and prepare for the day's work at the local bakery. His hands, dusted with flour, moved with practiced ease as he kneaded dough and shaped loaves. The bakery was a bustling hub in the small town, and Pablo took pride in his craft, knowing that his efforts brought joy and satisfaction to the community. The regulars knew him by name, and their friendly banter was the soundtrack to his summer days.\n",
      "\n",
      "As the seasons turned and the chill of winter set in, Pablo traded the warmth of the ovens for the quiet of the library. He was a diligent student, his mind as nimble as his baker's fingers, now turning pages and scribbling notes. Winter was a time for study, for burying himself in textbooks and lectures, preparing for a future that he hoped would be as fulfilling as his time at the bakery. The memories of summer's laughter and the scent of baking bread lingered, a gentle reminder of the balance he found between work and study, between the heat of the kitchen and the cool stillness of academic pursuit."
     ]
    }
   ],
   "source": [
    "chain = setup_and_retrieval | prompt |  chat_model | output_parser\n",
    "\n",
    "async for event in chain.astream(\"Tell me a story of 2 paragraphs\"):\n",
    "    print(event, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also use the method `.astream_log()` which outputs the all the intermediate steps of a chains, and allow us to debug the chain or to provide more info of the intermedite steps, like retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '840b382c-b91a-4cc9-96b9-f19ead866d56',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': ''})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'P'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'P'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ablo'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' was'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo was'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' working'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo was working'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' in'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo was working in'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo was working in a'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' bakery'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Pablo was working in a bakery'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' last'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Pablo was working in a bakery last'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' summer'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Pablo was working in a bakery last summer'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Pablo was working in a bakery last summer.'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})\n"
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream_log(\n",
    "    \"Where was Pablo working last summer?\", include_names=[\"Docs\"]\n",
    "):\n",
    "    print(\"-\" * 40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism\n",
    "LCEL chains can be parallelized when the input of one chain doesn't depend on the output of the other using `RunnableParallel` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = setup_and_retrieval | prompt |  chat_model | output_parser\n",
    "parallel_chain = RunnableParallel(question_rag_1=chain, question_rag_2=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 ms, sys: 7.42 ms, total: 41.2 ms\n",
      "Wall time: 1.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pablo works during the summer.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.invoke(\"During which season Pablo works? Give me a short answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.5 ms, sys: 4.02 ms, total: 48.5 ms\n",
      "Wall time: 958 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pablo studies during winter.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.invoke(\"During which season Pablo studies? Give me a short answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 89.3 ms, sys: 5.07 ms, total: 94.3 ms\n",
      "Wall time: 941 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question_rag_1': 'Winter.', 'question_rag_2': 'Pablo studies during winter.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "parallel_chain.invoke(\"During which season Pablo studies? Give me a short answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel + batch\n",
    "We can also run batch parallel operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 185 ms, sys: 13.9 ms, total: 199 ms\n",
      "Wall time: 1.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question_rag_1': 'Pablo studies during winter.',\n",
       "  'question_rag_2': 'Pablo studies during winter.'},\n",
       " {'question_rag_1': 'Pablo works during the summer.',\n",
       "  'question_rag_2': 'Pablo used to work in the summer.'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "parallel_chain.batch([\"During which season Pablo studies? Give me a short answer\", \"During which season Pablo works? Give me a short answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming and output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly stream Parsed JSON objects without getting JSON parsin error for having an uncompleted JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'cities': []}\n",
      "{'cities': [{}]}\n",
      "{'cities': [{'name': ''}]}\n",
      "{'cities': [{'name': 'Mad'}]}\n",
      "{'cities': [{'name': 'Madrid'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': ''}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'C'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Coc'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Mad'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madr'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrile'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': ''}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Val'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': ''}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Pa'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Val'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenc'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': ''}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Se'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': ''}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'G'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gaz'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazp'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': ''}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'S'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Com'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compost'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela', 'food': ''}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela', 'food': 'P'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela', 'food': 'Pul'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela', 'food': 'Pulpo'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela', 'food': 'Pulpo a'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela', 'food': 'Pulpo a la'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela', 'food': 'Pulpo a la Gal'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela', 'food': 'Pulpo a la Galleg'}]}\n",
      "{'cities': [{'name': 'Madrid', 'food': 'Cocido Madrileño'}, {'name': 'Valencia', 'food': 'Paella Valenciana'}, {'name': 'Sevilla', 'food': 'Gazpacho'}, {'name': 'Santiago de Compostela', 'food': 'Pulpo a la Gallega'}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = (\n",
    "    chat_model | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "async for text in chain.astream(\n",
    "    \"Output a list of the spanish cities Madrid, Valencia, Sevilla and Santiago de Compostela and their more popular food in JSON format. Use a dict with an outer key of 'cities' which contains a list of countries. Each country should have the key `name` and `food`\"\n",
    "):\n",
    "    print(text, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we want to include some extra step in the chain without breaking the functionality we have to use a generator function that implements a yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mad|Madrid|Val|Valencia|Se|Sevilla|S|Santiago|Santiago de|Santiago de Com|Santiago de Compost|Santiago de Compostela|"
     ]
    }
   ],
   "source": [
    "async def _extract_city_names_streaming(input_stream):\n",
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
    "    food_so_far = set()\n",
    "\n",
    "    async for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if \"cities\" not in input:\n",
    "            continue\n",
    "\n",
    "        cities = input[\"cities\"]\n",
    "\n",
    "        if not isinstance(cities, list):\n",
    "            continue\n",
    "\n",
    "        for city in cities:\n",
    "            name = city.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in food_so_far:\n",
    "                yield name\n",
    "                food_so_far.add(name)\n",
    "\n",
    "\n",
    "chain = chat_model | JsonOutputParser() | _extract_city_names_streaming\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"Output a list of the spanish cities Madrid, Valencia, Sevilla and Santiago de Compostela and their more popular food in JSON format. Use a dict with an outer key of 'cities' which contains a list of countries. Each country should have the key `name` and `food`\"\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical representation of LCEL chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            +---------------------------------+         \n",
      "            | Parallel<context,question>Input |         \n",
      "            +---------------------------------+         \n",
      "                    **               ***                \n",
      "                 ***                    ***             \n",
      "               **                          **           \n",
      "+----------------------+               +-------------+  \n",
      "| VectorStoreRetriever |               | Passthrough |  \n",
      "+----------------------+               +-------------+  \n",
      "                    **               ***                \n",
      "                      ***         ***                   \n",
      "                         **     **                      \n",
      "           +----------------------------------+         \n",
      "           | Parallel<context,question>Output |         \n",
      "           +----------------------------------+         \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                  +--------------------+                \n",
      "                  | ChatPromptTemplate |                \n",
      "                  +--------------------+                \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                    +-----------------+                 \n",
      "                    | AzureChatOpenAI |                 \n",
      "                    +-----------------+                 \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                    +-----------------+                 \n",
      "                    | StrOutputParser |                 \n",
      "                    +-----------------+                 \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                 +-----------------------+              \n",
      "                 | StrOutputParserOutput |              \n",
      "                 +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"pablo used to work in a bakery last summer\", \"during winter pablo studies\"],\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt |  chat_model | output_parser\n",
    "\n",
    "chain.get_graph().draw_mermaid_png(output_file_path=\"test_graph.png\")\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using @chain decorator\n",
    "We can turn functions into LCEL Runnables using the @chain decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Take an umbrella.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Answer with yes or no to if the weather is usually good in {city}\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"If the text is 'yes' answer: 'Apply suncream' otherwise answer: 'Take an umbrella'. Text: {answer_1}\")\n",
    "\n",
    "@chain\n",
    "def custom_chain(city):\n",
    "    prompt_val1 = prompt1.invoke({\"city\": city})\n",
    "    output1 = chat_model.invoke(prompt_val1)\n",
    "    parsed_output1 = StrOutputParser().invoke(output1)\n",
    "    chain2 = prompt2 | chat_model | StrOutputParser()\n",
    "    return chain2.invoke({\"answer_1\": parsed_output1})\n",
    "\n",
    "\n",
    "custom_chain.invoke(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the cloud apply suncream? Because it didn't want to get sunburned at the evaporate!\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_chain_output = RunnableParallel(\n",
    "    {\"context\": custom_chain}\n",
    ")\n",
    "prompt_final = ChatPromptTemplate.from_template(\"Based on context info make a very short joke with about weather: {context}\")\n",
    "\n",
    "chain_2 = prev_chain_output | prompt_final | chat_model | StrOutputParser()\n",
    "chain_2.invoke(\"Madrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining multiple chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most popular and traditional food in Jakarta is Nasi Goreng, which is Indonesian fried rice.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the capital of {country}?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"What is the most popular and traditional food in {capital}? Answer in one short sentence\"\n",
    ")\n",
    "\n",
    "chain1 = prompt1 | chat_model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"capital\": chain1}\n",
    "    | prompt2\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"country\": \"Indonesia\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
