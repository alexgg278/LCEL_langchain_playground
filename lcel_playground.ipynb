{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_RESOURCE\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_NAME\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_VERSION\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    "    max_tokens=4000,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.environ[\"AZURE_EMBEDDINGS_NAME\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_RESOURCE\"],\n",
    "    api_version=os.environ[\"AZURE_EMBEDDINGS_VERSION\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is LCEL?\n",
    "* LangChain expression language that allows to chain together runnable elements using the pipe `|` operator.\n",
    "* When chaining together runnable elements the output of the previous element .invoke() is passed to the next one.\n",
    "* A sequence of chained runnable elements is considered a Runnable Sequence and is itslef a Runnable that can be chained.\n",
    "\n",
    "# Why LangChain recommends the usage of LCEL?\n",
    "\n",
    "* Best for streaming: Get the best time-to-first-token. Get incremental chunks of output for chains at the same token rate as the given by the service provider (API).\n",
    "* Sync and async support.\n",
    "* Parallel execution: When some steps in a chain can be parallelized, they do it automatically.\n",
    "* More summarized code and more representative of the LangChain chains (easier to understand).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG example with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Projects/personal/LCEL_langchain_playground/.venv/lib/python3.10/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pablo was working in a bakery last summer.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"pablo used to work in a bakery last summer\", \"during winter pablo studies\"],\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt |  chat_model | output_parser\n",
    "\n",
    "chain.invoke(\"Where was Pablo working last summer?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need the final output of a LCEL chain we always have to use the `StrOutputParser`, otherwise we get all the response metadata as output. Using `StrOutputParser` is like only keeping `.content` when using `.invoke()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Pablo was working in a bakery last summer.', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_2f57f81c11', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-3a650416-240c-43ab-baec-8b90f21efe04-0', usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup_and_retrieval | prompt |  chat_model\n",
    "\n",
    "chain.invoke(\"Where was Pablo working last summer?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "It is done in the same way, we just use the runnable chain to run .astream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pablo spent his last summer enveloped in the warm, comforting aroma of freshly baked bread and pastries. Each morning, as the sun began to peek over the horizon, he would tie his apron and prepare for the day's work at the local bakery. His hands, dusted with flour, moved with practiced ease as he kneaded dough and shaped loaves. The bakery was a bustling hub in the small town, and Pablo took pride in his craft, knowing that his efforts brought joy and satisfaction to the community. The regulars knew him by name, and their friendly banter was the soundtrack to his summer days.\n",
      "\n",
      "As the seasons turned and the chill of winter set in, Pablo traded the warmth of the ovens for the quiet of the library. He was a diligent student, his mind as nimble as his baker's fingers, now turning pages and scribbling notes. Winter was a time for study, for burying himself in textbooks and lectures, preparing for a future that he hoped would be as fulfilling as his time at the bakery. The memories of summer's laughter and the scent of baking bread lingered, a gentle reminder of the balance he found between work and study, between the heat of the kitchen and the cool stillness of academic pursuit."
     ]
    }
   ],
   "source": [
    "chain = setup_and_retrieval | prompt |  chat_model | output_parser\n",
    "\n",
    "async for event in chain.astream(\"Tell me a story of 2 paragraphs\"):\n",
    "    print(event, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also use the method `.astream_log()` which outputs the all the intermediate steps of a chains, and allow us to debug the chain or to provide more info of the intermedite steps, like retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '840b382c-b91a-4cc9-96b9-f19ead866d56',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': ''})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'P'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'P'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ablo'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' was'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo was'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' working'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo was working'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' in'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo was working in'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Pablo was working in a'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' bakery'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Pablo was working in a bakery'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' last'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Pablo was working in a bakery last'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' summer'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Pablo was working in a bakery last summer'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Pablo was working in a bakery last summer.'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})\n"
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream_log(\n",
    "    \"Where was Pablo working last summer?\", include_names=[\"Docs\"]\n",
    "):\n",
    "    print(\"-\" * 40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism\n",
    "LCEL chains can be parallelized when the input of one chain doesn't depend on the output of the other using `RunnableParallel` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = setup_and_retrieval | prompt |  chat_model | output_parser\n",
    "parallel_chain = RunnableParallel(question_rag_1=chain, question_rag_2=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 ms, sys: 7.42 ms, total: 41.2 ms\n",
      "Wall time: 1.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pablo works during the summer.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.invoke(\"During which season Pablo works? Give me a short answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.5 ms, sys: 4.02 ms, total: 48.5 ms\n",
      "Wall time: 958 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pablo studies during winter.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.invoke(\"During which season Pablo studies? Give me a short answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 89.3 ms, sys: 5.07 ms, total: 94.3 ms\n",
      "Wall time: 941 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question_rag_1': 'Winter.', 'question_rag_2': 'Pablo studies during winter.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "parallel_chain.invoke(\"During which season Pablo studies? Give me a short answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel + batch\n",
    "We can also run batch parallel operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 185 ms, sys: 13.9 ms, total: 199 ms\n",
      "Wall time: 1.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question_rag_1': 'Pablo studies during winter.',\n",
       "  'question_rag_2': 'Pablo studies during winter.'},\n",
       " {'question_rag_1': 'Pablo works during the summer.',\n",
       "  'question_rag_2': 'Pablo used to work in the summer.'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "parallel_chain.batch([\"During which season Pablo studies? Give me a short answer\", \"During which season Pablo works? Give me a short answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
